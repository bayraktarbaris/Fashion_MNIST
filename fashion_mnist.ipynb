{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reproducible results...\n",
    "\n",
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "x_train = np.array(x_train, dtype = 'float32')\n",
    "x_test = np.array(x_test, dtype='float32')\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test= x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets (80% and 20% respectively)\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size=0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of classes, image shape and batch size\n",
    "num_classes = 10\n",
    "image_rows = 28\n",
    "image_cols = 28\n",
    "batch_size = 512\n",
    "image_shape = (image_rows, image_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape dataset\n",
    "x_train = x_train.reshape(x_train.shape[0], *image_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0], *image_shape)\n",
    "x_validate = x_validate.reshape(x_validate.shape[0], *image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1012 15:39:53.384567 140011726313280 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:529: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1012 15:39:53.388958 140011726313280 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4420: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1012 15:39:53.409588 140011726313280 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4255: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1012 15:39:53.413930 140011726313280 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1012 15:39:53.414945 140011726313280 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:136: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1012 15:39:53.429477 140011726313280 deprecation.py:506] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3721: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1012 15:39:53.585027 140011726313280 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4467: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define some models\n",
    "\n",
    "name = '1_Layer'\n",
    "cnn_model_1 = Sequential([\n",
    "    Conv2D(32, kernel_size=3, activation='relu', input_shape=image_shape, name='Conv2D-1'),\n",
    "    MaxPooling2D(pool_size=2, name='MaxPool'),\n",
    "    Dropout(0.2, name='Dropout'),\n",
    "    Flatten(name='flatten'),\n",
    "    Dense(32, activation='relu', name='Dense'),\n",
    "    Dense(10, activation='softmax', name='Output')\n",
    "], name=name)\n",
    "\n",
    "name = '2_Layer'\n",
    "cnn_model_2 = Sequential([\n",
    "    Conv2D(32, kernel_size=3, activation='relu', input_shape=image_shape, name='Conv2D-1'),\n",
    "    MaxPooling2D(pool_size=2, name='MaxPool'),\n",
    "    Dropout(0.2, name='Dropout-1'),\n",
    "    Conv2D(64, kernel_size=3, activation='relu', name='Conv2D-2'),\n",
    "    Dropout(0.25, name='Dropout-2'),\n",
    "    Flatten(name='flatten'),\n",
    "    Dense(64, activation='relu', name='Dense'),\n",
    "    Dense(10, activation='softmax', name='Output')\n",
    "], name=name)\n",
    "\n",
    "name='3_layer'\n",
    "cnn_model_3 = Sequential([\n",
    "    Conv2D(32, kernel_size=3, activation='relu', input_shape=image_shape, kernel_initializer='he_normal', \n",
    "           name='Conv2D-1'),\n",
    "    MaxPooling2D(pool_size=2, name='MaxPool'),\n",
    "    Dropout(0.25, name='Dropout-1'),\n",
    "    Conv2D(64, kernel_size=3, activation='relu', name='Conv2D-2'),\n",
    "    Dropout(0.25, name='Dropout-2'),\n",
    "    Conv2D(128, kernel_size=3, activation='relu', name='Conv2D-3'),\n",
    "    Dropout(0.4, name='Dropout-3'),\n",
    "    Flatten(name='flatten'),\n",
    "    Dense(128, activation='relu', name='Dense'),\n",
    "    Dropout(0.4, name='Dropout'),\n",
    "    Dense(10, activation='softmax', name='Output')\n",
    "], name=name)\n",
    "\n",
    "cnn_models = [cnn_model_1, cnn_model_2, cnn_model_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1012 15:39:58.181985 140011726313280 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1012 15:39:58.285103 140011726313280 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 23s 469us/step - loss: 0.7715 - acc: 0.7381 - val_loss: 0.4742 - val_acc: 0.8384\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 21s 430us/step - loss: 0.4386 - acc: 0.8465 - val_loss: 0.3873 - val_acc: 0.8705\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 21s 442us/step - loss: 0.3889 - acc: 0.8633 - val_loss: 0.3607 - val_acc: 0.8768\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 21s 439us/step - loss: 0.3610 - acc: 0.8729 - val_loss: 0.3272 - val_acc: 0.8911\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 23s 488us/step - loss: 0.3378 - acc: 0.8823 - val_loss: 0.3148 - val_acc: 0.8925\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 23s 488us/step - loss: 0.3219 - acc: 0.8872 - val_loss: 0.3194 - val_acc: 0.8894\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 22s 467us/step - loss: 0.3106 - acc: 0.8914 - val_loss: 0.3034 - val_acc: 0.8947\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 23s 478us/step - loss: 0.3023 - acc: 0.8930 - val_loss: 0.2839 - val_acc: 0.9022\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 23s 469us/step - loss: 0.2896 - acc: 0.8984 - val_loss: 0.2783 - val_acc: 0.9049\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 22s 450us/step - loss: 0.2817 - acc: 0.9003 - val_loss: 0.2828 - val_acc: 0.9011\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 22s 462us/step - loss: 0.2744 - acc: 0.9030 - val_loss: 0.2747 - val_acc: 0.9033\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 22s 466us/step - loss: 0.2659 - acc: 0.9054 - val_loss: 0.2699 - val_acc: 0.9050\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 23s 481us/step - loss: 0.2630 - acc: 0.9066 - val_loss: 0.2697 - val_acc: 0.9040\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 21s 445us/step - loss: 0.2595 - acc: 0.9079 - val_loss: 0.2591 - val_acc: 0.9098\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 21s 447us/step - loss: 0.2509 - acc: 0.9113 - val_loss: 0.2719 - val_acc: 0.9029\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 21s 446us/step - loss: 0.2466 - acc: 0.9130 - val_loss: 0.2519 - val_acc: 0.9120\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 21s 444us/step - loss: 0.2410 - acc: 0.9141 - val_loss: 0.2594 - val_acc: 0.9080\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 21s 446us/step - loss: 0.2389 - acc: 0.9141 - val_loss: 0.2470 - val_acc: 0.9146\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 21s 442us/step - loss: 0.2348 - acc: 0.9163 - val_loss: 0.2475 - val_acc: 0.9130\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 21s 443us/step - loss: 0.2290 - acc: 0.9177 - val_loss: 0.2437 - val_acc: 0.9148\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 21s 444us/step - loss: 0.2253 - acc: 0.9206 - val_loss: 0.2454 - val_acc: 0.9152\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 21s 443us/step - loss: 0.2230 - acc: 0.9208 - val_loss: 0.2373 - val_acc: 0.9168\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 23s 472us/step - loss: 0.2176 - acc: 0.9219 - val_loss: 0.2406 - val_acc: 0.9176\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 21s 442us/step - loss: 0.2147 - acc: 0.9233 - val_loss: 0.2356 - val_acc: 0.9183\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 21s 440us/step - loss: 0.2089 - acc: 0.9255 - val_loss: 0.2386 - val_acc: 0.9153\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 21s 439us/step - loss: 0.2084 - acc: 0.9256 - val_loss: 0.2403 - val_acc: 0.9163\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 21s 438us/step - loss: 0.2042 - acc: 0.9277 - val_loss: 0.2379 - val_acc: 0.9202\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 21s 441us/step - loss: 0.2006 - acc: 0.9280 - val_loss: 0.2302 - val_acc: 0.9204\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 21s 439us/step - loss: 0.1978 - acc: 0.9283 - val_loss: 0.2321 - val_acc: 0.9181\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 21s 438us/step - loss: 0.1934 - acc: 0.9299 - val_loss: 0.2325 - val_acc: 0.9208\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 21s 439us/step - loss: 0.1918 - acc: 0.9316 - val_loss: 0.2282 - val_acc: 0.9207\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 21s 438us/step - loss: 0.1897 - acc: 0.9312 - val_loss: 0.2297 - val_acc: 0.9184\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 21s 440us/step - loss: 0.1854 - acc: 0.9335 - val_loss: 0.2351 - val_acc: 0.9177\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 21s 435us/step - loss: 0.1839 - acc: 0.9337 - val_loss: 0.2269 - val_acc: 0.9210\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 21s 442us/step - loss: 0.1804 - acc: 0.9354 - val_loss: 0.2285 - val_acc: 0.9222\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 21s 439us/step - loss: 0.1788 - acc: 0.9350 - val_loss: 0.2275 - val_acc: 0.9202\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 21s 438us/step - loss: 0.1754 - acc: 0.9363 - val_loss: 0.2350 - val_acc: 0.9189\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 21s 438us/step - loss: 0.1717 - acc: 0.9382 - val_loss: 0.2303 - val_acc: 0.9217\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 21s 436us/step - loss: 0.1725 - acc: 0.9379 - val_loss: 0.2287 - val_acc: 0.9203\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 21s 435us/step - loss: 0.1678 - acc: 0.9394 - val_loss: 0.2293 - val_acc: 0.9196\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 21s 436us/step - loss: 0.1655 - acc: 0.9409 - val_loss: 0.2262 - val_acc: 0.9236\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 21s 436us/step - loss: 0.1655 - acc: 0.9397 - val_loss: 0.2295 - val_acc: 0.9206\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 21s 436us/step - loss: 0.1608 - acc: 0.9413 - val_loss: 0.2313 - val_acc: 0.9205\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 21s 435us/step - loss: 0.1581 - acc: 0.9425 - val_loss: 0.2296 - val_acc: 0.9232\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 21s 437us/step - loss: 0.1565 - acc: 0.9439 - val_loss: 0.2409 - val_acc: 0.9168\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 21s 432us/step - loss: 0.1545 - acc: 0.9444 - val_loss: 0.2307 - val_acc: 0.9203\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 21s 437us/step - loss: 0.1533 - acc: 0.9449 - val_loss: 0.2404 - val_acc: 0.9182\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 21s 436us/step - loss: 0.1494 - acc: 0.9458 - val_loss: 0.2268 - val_acc: 0.9243\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 21s 434us/step - loss: 0.1492 - acc: 0.9458 - val_loss: 0.2292 - val_acc: 0.9245\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 21s 441us/step - loss: 0.1463 - acc: 0.9470 - val_loss: 0.2274 - val_acc: 0.9259\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 42s 879us/step - loss: 0.7571 - acc: 0.7336 - val_loss: 0.5062 - val_acc: 0.8161\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 42s 865us/step - loss: 0.4774 - acc: 0.8263 - val_loss: 0.4256 - val_acc: 0.8497\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 42s 866us/step - loss: 0.4170 - acc: 0.8507 - val_loss: 0.3602 - val_acc: 0.8776\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 41s 855us/step - loss: 0.3801 - acc: 0.8646 - val_loss: 0.3356 - val_acc: 0.8847\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 41s 858us/step - loss: 0.3501 - acc: 0.8748 - val_loss: 0.3009 - val_acc: 0.8974\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 41s 856us/step - loss: 0.3281 - acc: 0.8833 - val_loss: 0.2871 - val_acc: 0.9003\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 41s 853us/step - loss: 0.3159 - acc: 0.8869 - val_loss: 0.2768 - val_acc: 0.9020\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 41s 857us/step - loss: 0.3036 - acc: 0.8901 - val_loss: 0.2665 - val_acc: 0.9073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 41s 854us/step - loss: 0.2960 - acc: 0.8941 - val_loss: 0.2737 - val_acc: 0.9027\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 41s 853us/step - loss: 0.2802 - acc: 0.8988 - val_loss: 0.2528 - val_acc: 0.9113\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 41s 847us/step - loss: 0.2658 - acc: 0.9024 - val_loss: 0.2497 - val_acc: 0.9093\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 41s 857us/step - loss: 0.2587 - acc: 0.9056 - val_loss: 0.2504 - val_acc: 0.9099\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 41s 852us/step - loss: 0.2489 - acc: 0.9081 - val_loss: 0.2382 - val_acc: 0.9139\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 41s 852us/step - loss: 0.2387 - acc: 0.9121 - val_loss: 0.2331 - val_acc: 0.9162\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 41s 853us/step - loss: 0.2315 - acc: 0.9152 - val_loss: 0.2286 - val_acc: 0.9180\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 41s 854us/step - loss: 0.2233 - acc: 0.9180 - val_loss: 0.2315 - val_acc: 0.9177\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 41s 850us/step - loss: 0.2175 - acc: 0.9202 - val_loss: 0.2324 - val_acc: 0.9157\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 40s 836us/step - loss: 0.2104 - acc: 0.9223 - val_loss: 0.2188 - val_acc: 0.9203\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 41s 845us/step - loss: 0.2048 - acc: 0.9236 - val_loss: 0.2174 - val_acc: 0.9243\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 41s 858us/step - loss: 0.1959 - acc: 0.9275 - val_loss: 0.2363 - val_acc: 0.9162\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 41s 855us/step - loss: 0.1917 - acc: 0.9289 - val_loss: 0.2150 - val_acc: 0.9237\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 41s 847us/step - loss: 0.1867 - acc: 0.9305 - val_loss: 0.2123 - val_acc: 0.9256\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 41s 856us/step - loss: 0.1786 - acc: 0.9338 - val_loss: 0.2150 - val_acc: 0.9235\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 41s 852us/step - loss: 0.1737 - acc: 0.9364 - val_loss: 0.2127 - val_acc: 0.9241\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 41s 848us/step - loss: 0.1682 - acc: 0.9376 - val_loss: 0.2182 - val_acc: 0.9239\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 41s 845us/step - loss: 0.1639 - acc: 0.9392 - val_loss: 0.2224 - val_acc: 0.9210\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 41s 854us/step - loss: 0.1660 - acc: 0.9379 - val_loss: 0.2119 - val_acc: 0.9248\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 41s 850us/step - loss: 0.1579 - acc: 0.9408 - val_loss: 0.2085 - val_acc: 0.9293\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 41s 852us/step - loss: 0.1464 - acc: 0.9440 - val_loss: 0.2121 - val_acc: 0.9280\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 41s 851us/step - loss: 0.1423 - acc: 0.9462 - val_loss: 0.2141 - val_acc: 0.9262\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 41s 853us/step - loss: 0.1393 - acc: 0.9485 - val_loss: 0.2129 - val_acc: 0.9274\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 41s 851us/step - loss: 0.1361 - acc: 0.9494 - val_loss: 0.2117 - val_acc: 0.9295\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 41s 850us/step - loss: 0.1369 - acc: 0.9481 - val_loss: 0.2142 - val_acc: 0.9306\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 41s 851us/step - loss: 0.1307 - acc: 0.9517 - val_loss: 0.2346 - val_acc: 0.9217\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 41s 858us/step - loss: 0.1245 - acc: 0.9531 - val_loss: 0.2198 - val_acc: 0.9277\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 41s 850us/step - loss: 0.1218 - acc: 0.9542 - val_loss: 0.2200 - val_acc: 0.9278\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 41s 852us/step - loss: 0.1154 - acc: 0.9562 - val_loss: 0.2231 - val_acc: 0.9277\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 41s 850us/step - loss: 0.1098 - acc: 0.9592 - val_loss: 0.2164 - val_acc: 0.9303\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 41s 852us/step - loss: 0.1126 - acc: 0.9580 - val_loss: 0.2233 - val_acc: 0.9298\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 41s 847us/step - loss: 0.1061 - acc: 0.9601 - val_loss: 0.2260 - val_acc: 0.9293\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 41s 848us/step - loss: 0.1083 - acc: 0.9589 - val_loss: 0.2337 - val_acc: 0.9290\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 41s 853us/step - loss: 0.1012 - acc: 0.9618 - val_loss: 0.2584 - val_acc: 0.9208\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 41s 851us/step - loss: 0.0994 - acc: 0.9630 - val_loss: 0.2289 - val_acc: 0.9288\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 41s 852us/step - loss: 0.0968 - acc: 0.9637 - val_loss: 0.2397 - val_acc: 0.9300\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 41s 849us/step - loss: 0.0955 - acc: 0.9641 - val_loss: 0.2258 - val_acc: 0.9317\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 41s 848us/step - loss: 0.0899 - acc: 0.9662 - val_loss: 0.2373 - val_acc: 0.9307\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 41s 849us/step - loss: 0.0897 - acc: 0.9663 - val_loss: 0.2378 - val_acc: 0.9301\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 41s 850us/step - loss: 0.0859 - acc: 0.9668 - val_loss: 0.2431 - val_acc: 0.9303\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 41s 853us/step - loss: 0.0799 - acc: 0.9691 - val_loss: 0.2491 - val_acc: 0.9268\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 41s 854us/step - loss: 0.0819 - acc: 0.9690 - val_loss: 0.2471 - val_acc: 0.9275\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.7380 - acc: 0.7324 - val_loss: 0.4287 - val_acc: 0.8413\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 85s 2ms/step - loss: 0.4620 - acc: 0.8302 - val_loss: 0.3468 - val_acc: 0.8767\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 85s 2ms/step - loss: 0.4067 - acc: 0.8527 - val_loss: 0.3051 - val_acc: 0.8918\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 85s 2ms/step - loss: 0.3673 - acc: 0.8656 - val_loss: 0.2822 - val_acc: 0.8953\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.3434 - acc: 0.8749 - val_loss: 0.2773 - val_acc: 0.8970\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.3242 - acc: 0.8803 - val_loss: 0.2603 - val_acc: 0.9052\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.3063 - acc: 0.8894 - val_loss: 0.2481 - val_acc: 0.9127\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2971 - acc: 0.8901 - val_loss: 0.2514 - val_acc: 0.9090\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 84s 2ms/step - loss: 0.2863 - acc: 0.8933 - val_loss: 0.2379 - val_acc: 0.9141\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2793 - acc: 0.8966 - val_loss: 0.2394 - val_acc: 0.9142\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2721 - acc: 0.9004 - val_loss: 0.2404 - val_acc: 0.9152\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2562 - acc: 0.9048 - val_loss: 0.2317 - val_acc: 0.9168\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2520 - acc: 0.9049 - val_loss: 0.2308 - val_acc: 0.9187\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2487 - acc: 0.9045 - val_loss: 0.2248 - val_acc: 0.9192\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2422 - acc: 0.9091 - val_loss: 0.2265 - val_acc: 0.9199\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2346 - acc: 0.9112 - val_loss: 0.2249 - val_acc: 0.9209\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2276 - acc: 0.9128 - val_loss: 0.2399 - val_acc: 0.9174\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2255 - acc: 0.9140 - val_loss: 0.2271 - val_acc: 0.9229\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2193 - acc: 0.9145 - val_loss: 0.2341 - val_acc: 0.9224\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2238 - acc: 0.9146 - val_loss: 0.2275 - val_acc: 0.9217\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2159 - acc: 0.9182 - val_loss: 0.2331 - val_acc: 0.9214\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2099 - acc: 0.9192 - val_loss: 0.2264 - val_acc: 0.9208\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2104 - acc: 0.9185 - val_loss: 0.2199 - val_acc: 0.9214\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.2020 - acc: 0.9228 - val_loss: 0.2245 - val_acc: 0.9273\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1957 - acc: 0.9261 - val_loss: 0.2287 - val_acc: 0.9260\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1956 - acc: 0.9240 - val_loss: 0.2367 - val_acc: 0.9238\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1958 - acc: 0.9243 - val_loss: 0.2292 - val_acc: 0.9248\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1921 - acc: 0.9254 - val_loss: 0.2363 - val_acc: 0.9251\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 84s 2ms/step - loss: 0.1901 - acc: 0.9263 - val_loss: 0.2248 - val_acc: 0.9260\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1834 - acc: 0.9282 - val_loss: 0.2327 - val_acc: 0.9241\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1831 - acc: 0.9289 - val_loss: 0.2359 - val_acc: 0.9275\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1861 - acc: 0.9288 - val_loss: 0.2285 - val_acc: 0.9237\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1829 - acc: 0.9282 - val_loss: 0.2345 - val_acc: 0.9215\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1779 - acc: 0.9310 - val_loss: 0.2316 - val_acc: 0.9273\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1793 - acc: 0.9317 - val_loss: 0.2265 - val_acc: 0.9280\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1776 - acc: 0.9318 - val_loss: 0.2370 - val_acc: 0.9242\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1751 - acc: 0.9334 - val_loss: 0.2367 - val_acc: 0.9284\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1742 - acc: 0.9324 - val_loss: 0.2350 - val_acc: 0.9273\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1768 - acc: 0.9323 - val_loss: 0.2400 - val_acc: 0.9268\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1692 - acc: 0.9349 - val_loss: 0.2337 - val_acc: 0.9273\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1639 - acc: 0.9362 - val_loss: 0.2374 - val_acc: 0.9273\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 84s 2ms/step - loss: 0.1644 - acc: 0.9360 - val_loss: 0.2385 - val_acc: 0.9259\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1621 - acc: 0.9376 - val_loss: 0.2375 - val_acc: 0.9273\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1569 - acc: 0.9384 - val_loss: 0.2481 - val_acc: 0.9275\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1617 - acc: 0.9375 - val_loss: 0.2434 - val_acc: 0.9270\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1615 - acc: 0.9374 - val_loss: 0.2369 - val_acc: 0.9287\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1589 - acc: 0.9405 - val_loss: 0.2393 - val_acc: 0.9269\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1539 - acc: 0.9406 - val_loss: 0.2349 - val_acc: 0.9315\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1563 - acc: 0.9395 - val_loss: 0.2393 - val_acc: 0.9291\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.1479 - acc: 0.9415 - val_loss: 0.2452 - val_acc: 0.9323\n"
     ]
    }
   ],
   "source": [
    "# Fit models\n",
    "history_dict = {}\n",
    "for model in cnn_models:\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=Adam(lr=0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=50, verbose=1,\n",
    "        validation_data=(x_validate, y_validate)\n",
    "    )\n",
    "    \n",
    "    history_dict[model.name] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: 1_Layer\n",
      "Test Loss: 0.2559\n",
      "Test Accuracy: 0.9137\n",
      "Model Name: 2_Layer\n",
      "Test Loss: 0.2795\n",
      "Test Accuracy: 0.9205\n",
      "Model Name: 3_layer\n",
      "Test Loss: 0.2927\n",
      "Test Accuracy: 0.9197\n"
     ]
    }
   ],
   "source": [
    "# Get test loss and accuracy\n",
    "for model in cnn_models:\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Model Name: {}'.format(model.name))\n",
    "    print('Test Loss: {:.4f}'.format(score[0]))\n",
    "    print('Test Accuracy: {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot losses and accuracies by epoch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig,(ax1, ax2)=plt.subplots(2, figsize=(8, 6))\n",
    "for history in history_dict:\n",
    "    val_acc = history_dict[history].history['val_acc']\n",
    "    val_loss = history_dict[history].history['val_loss']\n",
    "    ax1.plot(val_acc, label=history)\n",
    "    ax2.plot(val_loss, label=history)\n",
    "\n",
    "ax1.set_ylabel('Validation Accuracy')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in cnn_models:\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
