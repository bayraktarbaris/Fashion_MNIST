{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reproducible results...\n",
    "\n",
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets (80% and 20% respectively)\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size=0.2,random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of classes, image shape and batch size\n",
    "num_classes = 10\n",
    "image_rows = 28\n",
    "image_cols = 28\n",
    "batch_size = 512\n",
    "image_shape = (image_rows, image_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape dataset\n",
    "x_train = x_train.reshape(x_train.shape[0], *image_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0], *image_shape)\n",
    "x_validate = x_validate.reshape(x_validate.shape[0], *image_shape)\n",
    "\n",
    "# TODO: try normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1012 14:01:04.029224 140478202677056 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:529: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1012 14:01:04.037150 140478202677056 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4420: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1012 14:01:04.076004 140478202677056 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4255: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1012 14:01:04.084139 140478202677056 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1012 14:01:04.085345 140478202677056 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:136: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1012 14:01:04.100027 140478202677056 deprecation.py:506] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3721: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1012 14:01:04.267244 140478202677056 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4467: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define some models\n",
    "\n",
    "name = '1_Layer'\n",
    "cnn_model_1 = Sequential([\n",
    "    Conv2D(32, kernel_size=3, activation='relu', input_shape=image_shape, name='Conv2D-1'),\n",
    "    MaxPooling2D(pool_size=2, name='MaxPool'),\n",
    "    Dropout(0.2, name='Dropout'),\n",
    "    Flatten(name='flatten'),\n",
    "    Dense(32, activation='relu', name='Dense'),\n",
    "    Dense(10, activation='softmax', name='Output')\n",
    "], name=name)\n",
    "\n",
    "name = '2_Layer'\n",
    "cnn_model_2 = Sequential([\n",
    "    Conv2D(32, kernel_size=3, activation='relu', input_shape=image_shape, name='Conv2D-1'),\n",
    "    MaxPooling2D(pool_size=2, name='MaxPool'),\n",
    "    Dropout(0.2, name='Dropout-1'),\n",
    "    Conv2D(64, kernel_size=3, activation='relu', name='Conv2D-2'),\n",
    "    Dropout(0.25, name='Dropout-2'),\n",
    "    Flatten(name='flatten'),\n",
    "    Dense(64, activation='relu', name='Dense'),\n",
    "    Dense(10, activation='softmax', name='Output')\n",
    "], name=name)\n",
    "\n",
    "name='3_layer'\n",
    "cnn_model_3 = Sequential([\n",
    "    Conv2D(32, kernel_size=3, activation='relu', input_shape=image_shape, kernel_initializer='he_normal', \n",
    "           name='Conv2D-1'),\n",
    "    MaxPooling2D(pool_size=2, name='MaxPool'),\n",
    "    Dropout(0.25, name='Dropout-1'),\n",
    "    Conv2D(64, kernel_size=3, activation='relu', name='Conv2D-2'),\n",
    "    Dropout(0.25, name='Dropout-2'),\n",
    "    Conv2D(128, kernel_size=3, activation='relu', name='Conv2D-3'),\n",
    "    Dropout(0.4, name='Dropout-3'),\n",
    "    Flatten(name='flatten'),\n",
    "    Dense(128, activation='relu', name='Dense'),\n",
    "    Dropout(0.4, name='Dropout'),\n",
    "    Dense(10, activation='softmax', name='Output')\n",
    "], name=name)\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(filters=32,kernel_size=3, activation='relu', input_shape=image_shape),\n",
    "    MaxPooling2D(pool_size=2),# down sampling the output instead of 28*28 it is 14*14\n",
    "    Dropout(0.2),\n",
    "    Flatten(), # flatten out the layers\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(10,activation = 'softmax')\n",
    "    \n",
    "])\n",
    "\n",
    "cnn_models = [cnn_model_1, cnn_model_2, cnn_model_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1012 14:01:18.080354 140478202677056 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1012 14:01:18.197843 140478202677056 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 30s 632us/step - loss: 6.1264 - acc: 0.6019 - val_loss: 4.0672 - val_acc: 0.7211\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 30s 624us/step - loss: 1.4495 - acc: 0.7844 - val_loss: 0.4418 - val_acc: 0.8572\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 27s 572us/step - loss: 0.4256 - acc: 0.8564 - val_loss: 0.3722 - val_acc: 0.8768\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 30s 623us/step - loss: 0.3538 - acc: 0.8748 - val_loss: 0.3445 - val_acc: 0.8854\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 27s 566us/step - loss: 0.3142 - acc: 0.8861 - val_loss: 0.3252 - val_acc: 0.8908\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 27s 568us/step - loss: 0.2848 - acc: 0.8953 - val_loss: 0.3193 - val_acc: 0.8912\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 24s 502us/step - loss: 0.2657 - acc: 0.9015 - val_loss: 0.3090 - val_acc: 0.8936\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 24s 504us/step - loss: 0.2495 - acc: 0.9076 - val_loss: 0.2998 - val_acc: 0.8977\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 28s 575us/step - loss: 0.2344 - acc: 0.9123 - val_loss: 0.3033 - val_acc: 0.8970\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 27s 562us/step - loss: 0.2254 - acc: 0.9156 - val_loss: 0.2993 - val_acc: 0.8999\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 28s 584us/step - loss: 0.2097 - acc: 0.9201 - val_loss: 0.3040 - val_acc: 0.9008\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 26s 542us/step - loss: 0.2015 - acc: 0.9227 - val_loss: 0.3084 - val_acc: 0.9010\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 28s 584us/step - loss: 0.1959 - acc: 0.9251 - val_loss: 0.3068 - val_acc: 0.9006\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 34s 713us/step - loss: 0.1848 - acc: 0.9285 - val_loss: 0.3106 - val_acc: 0.9011\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 37s 774us/step - loss: 0.1781 - acc: 0.9319 - val_loss: 0.3196 - val_acc: 0.9024\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 39s 804us/step - loss: 0.1759 - acc: 0.9332 - val_loss: 0.3170 - val_acc: 0.9033\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 38s 794us/step - loss: 0.1677 - acc: 0.9367 - val_loss: 0.3162 - val_acc: 0.9019\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 27s 554us/step - loss: 0.1647 - acc: 0.9369 - val_loss: 0.3177 - val_acc: 0.9053\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 30s 616us/step - loss: 0.1588 - acc: 0.9382 - val_loss: 0.3179 - val_acc: 0.9037\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 29s 597us/step - loss: 0.1538 - acc: 0.9414 - val_loss: 0.3248 - val_acc: 0.9043\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 26s 535us/step - loss: 0.1477 - acc: 0.9432 - val_loss: 0.3337 - val_acc: 0.9045\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 26s 545us/step - loss: 0.1433 - acc: 0.9454 - val_loss: 0.3336 - val_acc: 0.9053\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 25s 526us/step - loss: 0.1388 - acc: 0.9473 - val_loss: 0.3370 - val_acc: 0.9080\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 26s 537us/step - loss: 0.1378 - acc: 0.9474 - val_loss: 0.3431 - val_acc: 0.9037\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 26s 532us/step - loss: 0.1337 - acc: 0.9494 - val_loss: 0.3401 - val_acc: 0.9027\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 25s 529us/step - loss: 0.1311 - acc: 0.9500 - val_loss: 0.3500 - val_acc: 0.9047\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 25s 517us/step - loss: 0.1295 - acc: 0.9514 - val_loss: 0.3488 - val_acc: 0.9033\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 25s 517us/step - loss: 0.1261 - acc: 0.9524 - val_loss: 0.3628 - val_acc: 0.9046\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 26s 549us/step - loss: 0.1194 - acc: 0.9549 - val_loss: 0.3555 - val_acc: 0.9035\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 26s 548us/step - loss: 0.1189 - acc: 0.9544 - val_loss: 0.3614 - val_acc: 0.9017\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 29s 610us/step - loss: 0.1181 - acc: 0.9543 - val_loss: 0.3681 - val_acc: 0.9035\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 27s 559us/step - loss: 0.1178 - acc: 0.9546 - val_loss: 0.3897 - val_acc: 0.9053\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 25s 529us/step - loss: 0.1132 - acc: 0.9581 - val_loss: 0.3879 - val_acc: 0.9017\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 27s 570us/step - loss: 0.1099 - acc: 0.9577 - val_loss: 0.3906 - val_acc: 0.9013\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 27s 558us/step - loss: 0.1072 - acc: 0.9584 - val_loss: 0.3874 - val_acc: 0.9050\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 27s 563us/step - loss: 0.1049 - acc: 0.9602 - val_loss: 0.3911 - val_acc: 0.9069\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 26s 543us/step - loss: 0.1052 - acc: 0.9605 - val_loss: 0.3976 - val_acc: 0.9033\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 24s 506us/step - loss: 0.1038 - acc: 0.9606 - val_loss: 0.3940 - val_acc: 0.9064\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 26s 549us/step - loss: 0.1070 - acc: 0.9588 - val_loss: 0.3902 - val_acc: 0.9042\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 34s 702us/step - loss: 0.0993 - acc: 0.9629 - val_loss: 0.4035 - val_acc: 0.9022\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 24s 507us/step - loss: 0.0994 - acc: 0.9624 - val_loss: 0.4099 - val_acc: 0.9021\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 27s 558us/step - loss: 0.0981 - acc: 0.9631 - val_loss: 0.4082 - val_acc: 0.9049\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 26s 536us/step - loss: 0.0923 - acc: 0.9646 - val_loss: 0.4180 - val_acc: 0.9040\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 26s 539us/step - loss: 0.0963 - acc: 0.9641 - val_loss: 0.4192 - val_acc: 0.9032\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 25s 516us/step - loss: 0.0921 - acc: 0.9658 - val_loss: 0.4194 - val_acc: 0.9038\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 25s 514us/step - loss: 0.0924 - acc: 0.9653 - val_loss: 0.4152 - val_acc: 0.9045\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 26s 548us/step - loss: 0.0892 - acc: 0.9667 - val_loss: 0.4230 - val_acc: 0.9029\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 26s 549us/step - loss: 0.0881 - acc: 0.9665 - val_loss: 0.4408 - val_acc: 0.9023\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 26s 546us/step - loss: 0.0866 - acc: 0.9673 - val_loss: 0.4226 - val_acc: 0.9067\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 26s 537us/step - loss: 0.0890 - acc: 0.9668 - val_loss: 0.4359 - val_acc: 0.9027\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 55s 1ms/step - loss: 11.4120 - acc: 0.2899 - val_loss: 12.1950 - val_acc: 0.2432\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 59s 1ms/step - loss: 11.7925 - acc: 0.2682 - val_loss: 11.7286 - val_acc: 0.2723\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 58s 1ms/step - loss: 11.8797 - acc: 0.2629 - val_loss: 11.8268 - val_acc: 0.2661\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 57s 1ms/step - loss: 12.2758 - acc: 0.2383 - val_loss: 13.0077 - val_acc: 0.1929\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 55s 1ms/step - loss: 12.9015 - acc: 0.1996 - val_loss: 12.9925 - val_acc: 0.1939\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 57s 1ms/step - loss: 12.8921 - acc: 0.2001 - val_loss: 12.9898 - val_acc: 0.1941\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 55s 1ms/step - loss: 12.8908 - acc: 0.2002 - val_loss: 12.9898 - val_acc: 0.1941\n",
      "Epoch 8/50\n",
      "37888/48000 [======================>.......] - ETA: 11s - loss: 12.9011 - acc: 0.1996"
     ]
    }
   ],
   "source": [
    "# Fit models\n",
    "history_dict = {}\n",
    "for model in cnn_models:\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=Adam(), # Adam(lr=0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=50, verbose=1,\n",
    "        validation_data=(x_validate, y_validate)\n",
    "    )\n",
    "    \n",
    "    history_dict[model.name] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test loss and accuracy\n",
    "score = cnn_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss : {:.4f}'.format(score[0]))\n",
    "print('Test Accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses and accuracies by epoch\n",
    "fig,(ax1, ax2)=plt.subplots(2, figsize=(8, 6))\n",
    "for history in history_dict:\n",
    "    val_acc = history_dict[history].history['val_acc']\n",
    "    val_loss = history_dict[history].history['val_loss']\n",
    "    ax1.plot(val_acc, label=history)\n",
    "    ax2.plot(val_loss, label=history)\n",
    "\n",
    "ax1.set_ylabel('Validation Accuracy')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
